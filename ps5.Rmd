---
title: "Problem Set 5"
author: "Chanuwas (New) Aswamenakul"
date: "2/19/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

For this problem set and the next, you will use a dataset (erp_data.csv) from an EEG (electroencephalogram) experiment. Participants read sentences while electrical activity was recorded from their scalp. EEG activity time-locked to a specific critical stimulus (e.g., a word) is called an Event-Related Potential (ERP). In the electrophysiology literature, some time windows within ERPs have been associated with specific cognitive computations. For example, the “P600” a positive deflection of the ERP waveform, typically measured between 600 and 800ms after the stimulus, is thought to index some kind of error detection process.

Some of the sentences in this experiment contained errors in the last word (e.g., typos like “He always told a good anecdotes”) and some of them just have words that don’t make sense in the context of the sentence (“He always told a good horse”). The Condition column contains information about what kind of error it is (details not important for our purposes). Mean ERP amplitudes are in the MeanAmp column.

The sentences in this experiment were also given to an independent set of participants who were asked to correct the last word of the sentence if they thought it contained an error. The results of that norming study are in erp_norms.csv. The Intended Completion column counts how many times participants corrected the word to the intended word (e.g., anecdote).

**We want to know if the P600 amplitude is related to the probability that the intended word can be recovered.** To answer this question you will first need to do some data wrangling before you can jump into the analysis.

1) Read in the two datasets erp_data.csv and erp_norms.csv. From erp_data.csv, filter out data from the “n4” time window, leaving only “p6” data. Also filter out trials with artifacts (i.e., trials where the data aren’t good because of blinks or eye-movements) marked as 1 in the artifact column. Finally, keep only data from electrodes that are centro-parietal (where P600 effects are known to be largest from the literature). This includes C3, Cz, C4, CP1, CP2, P3, Pz, and P4.

```{r}
erp.df <- read_csv(file.path("data", "erp_data.csv"))
erp.df %>% glimpse()

erp.df <- erp.df %>% 
  filter(time_window == "p6",
         artifact != 1,
         electrode %in% c("C3", "Cz", "C4", "CP1", "CP2", "P3", "Pz", "P4"))

erp.norm <- read_csv(file.path("data", "erp_norms.csv"))
erp.norm %>% glimpse()

colnames(erp.norm) <- colnames(erp.norm) %>% str_replace(" ", "_")
```

2) Compute average “p6” amplitudes for each item and each condition.

```{r}
summarized.erp <- erp.df %>% 
  group_by(itemNum, condition) %>% 
  summarize(p6.amp = mean(meanAmp))

summarized.erp
```

3) In the summarized erp_data, for each item, calculate the difference in amplitude between each of the other three conditions and control. In other words, you will need to pivot to a wide format so that each condition gets a column with amplitude values and subtract the control amplitude from each.

```{r}
relative.amp <- summarized.erp %>% 
  pivot_wider(id_cols = itemNum,
              names_from = condition,
              values_from = p6.amp) %>% 
  mutate(Sem = Sem - Control,
         SemCrit = SemCrit - Control,
         Synt = Synt - Control)

relative.amp
```

4) Add a Percent_recovered column to the erp_norms dataframe (dividing Intended word counts by Total Completions)

```{r}
erp.norm <- erp.norm %>% 
  mutate(Percent_recovered = Intended_Completion / Total_Completions)

erp.norm
```

5) Finally, join the norms to the summarized erp data by item number (hint: slightly different column names) and condition. (You may need to pivot back to a longer format first.)

```{r}
erp.with.norm <- relative.amp %>% 
  pivot_longer(cols = Control:Synt,
               names_to = "condition",
               values_to = "p6.amp") %>% 
  inner_join(erp.norm, by = c("itemNum" = "Item", "condition" = "Condition"))
```

6) Plot P600 amplitude (relative to control) by item over Percent_recovered.

```{r}
erp.with.norm %>% 
  ggplot(aes(x = Percent_recovered, y = p6.amp)) +
  geom_point()
```

7) Conduct a linear regression analysis using lm() to address the research question (in bold above).

```{r}
recov.erp.model <- lm(p6.amp ~ Percent_recovered, data = erp.with.norm)
summary(recov.erp.model)
```

8) Comment on the following pitfalls of regression. In other words, if they apply, say how you checked. If they don’t apply say why they don’t apply.

  8.1 Non-linearity of the response-predictor relationship(s)
    
  **Answer**: We can check non-linearity of the response-predictor relationship using a residual plot. If the residuals deviate from 0 systematically, it can be a sign of non-linear relationship between response and predictor variables. The below residual plot shows that it is not the case since we see no discernible pattern.
    
```{r}
plot(recov.erp.model, 1)
```

  8.2 Non-Normality of the distribution of error terms
    
  **Answer**: We can check normality of the distribution of error terms by using a Q-Q plot. If the data mostly fall along 45-degree angle staight line, then the error term is normally distributed. As shown in the plot below, there's no alarming deviation from the normality line.
    
```{r}
plot(recov.erp.model, 2)
```
  
  8.3 Non-constant variance of error terms
  **Answer**: We can also check non-constant variance of error terms (heteroskedasticity) using a residual plot. If there is higher dispersion of the residuals for certain range of fitted values than other, that indicates that the error terms have non-constant variance. But we don't notice any irregular dispersion of the residuals here.
  
```{r}
plot(recov.erp.model, 1)
```
  
  8.4 Correlation of error terms
  
  **Answer**: We can check the correlation of error terms using a residual plot. Because we observe no tracking in the residual plot below. There's no evidence that the error terms are correlated.
  
```{r}
plot(resid(recov.erp.model))
```
  
  8.5 Outliers
  
  **Answer**: We can detect outliers by standardizing residuals and check if there are standardized residuals whose values exceed a threshold, which we will use -3 and 3 in this case. From the plot below, there is one data point that is close to the value -3. So, that is a potential outlier.
  
```{r}
plot(recov.erp.model, 5)
```
  
  8.6 High-leverage points
  
  **Answer**: We can detect high-leverage points by calculating Cook's distance or leverage statistics. As shown in the plot below, there are a few points that have slightly high values of Cook's distance and leverage statistics relative to the rest of the data. However, those values are still not quite large enough to disproportionately affect the model.
  
```{r}
plot(recov.erp.model, 6)
```
  
  8.7 Collinearity
  
  **Answer**: Collinearity is not applicable to our analysis here because we have only one predictor variable.

9) Summarize the results of your analysis in full sentences (as you would in the results section of a paper).

**Answer**: The estimate of linear regression shows that the coefficient of percentage of recovered word is statistically significant and different from zero. Hence, we conclude that P600 amplitude is positively related to the probability that the intended word can be recovered.